# Advanced Dimensionality Reduction Assignment

##  [**Video Walkthrough**](HTTP://youtube.com)

---

## Colab Notebooks

### 1. Image Dataset
- [Colab 1 - Image Data](INSERT_COLAB_1_LINK)


---

### 2. Tabular Dataset
- [Colab 2 - Tabular Data](INSERT_COLAB_2_LINK)


---

### 3. Databricks Implementation
- [Colab 3 - Databricks](INSERT_COLAB_3_LINK)

---

## Introduction
Dimensionality reduction is a crucial step in data analysis and machine learning, particularly when working with high-dimensional datasets. It involves reducing the number of input variables while retaining the essential structure and patterns of the data. This process helps improve computational efficiency, reduces storage requirements, and enhances data visualization.

This project explores **Advanced Dimensionality Reduction (ADR)** techniques, showcasing their application to various datasets (image, tabular, and medical) using Python in Google Colab and Databricks. The notebooks and associated walkthroughs aim to demonstrate the utility, effectiveness, and computational trade-offs of these techniques.

---

## Overview of Dimensionality Reduction Techniques
### 1. Locally Linear Embedding (LLE)
- Captures non-linear structures in high-dimensional data by preserving local neighborhood relationships in the reduced space.

### 2. t-SNE (t-Distributed Stochastic Neighbor Embedding)
- A non-linear technique designed for interactive visualization, effectively highlighting clusters in data by preserving local relationships.

### 3. ISOMAP
- Extends Multi-Dimensional Scaling (MDS) by preserving global geometric structures while embedding data into a lower-dimensional space.

### 4. UMAP (Uniform Manifold Approximation and Projection)
- A computationally efficient technique that focuses on interactive visualization and maintaining local and global data structures.

### 5. MDS (Multi-Dimensional Scaling)
- Focuses on preserving the pairwise distance structure of the data when mapping to a lower-dimensional representation.

### 6. Randomized PCA
- Accelerates traditional PCA computations by using randomization techniques, making it suitable for large datasets.

### 7. Kernel PCA
- Extends PCA by incorporating kernel methods, enabling non-linear dimensionality reduction.

### 8. Incremental PCA
- A variation of PCA designed for large datasets, processing data in batches to minimize memory usage.

### 9. Factor Analysis
- A statistical method that identifies latent variables (factors) to explain observed data structures.

### 10. Autoencoders
- Neural network-based methods that learn compressed representations of input data, enabling non-linear dimensionality reduction.

---

## Results and Analysis
Each notebook includes:
1. A detailed walkthrough of the code.
2. Commentary on the results of each dimensionality reduction technique.
3. Comparison of techniques based on:
   - Computational efficiency
   - Effectiveness in preserving data structure
   - Visual clarity in lower dimensions.

---

## Acknowledgments
Special thanks to:
- [Hands-On Machine Learning by Aurélien Géron](https://github.com/ageron/handson-ml2)
- Dimensionality Reduction course resources and slides.
- [Kaggle datasets](https://www.kaggle.com/) for providing data.

---

## Author
**Rushabh Runwal**  
*Data Mining Course Assignment*  
